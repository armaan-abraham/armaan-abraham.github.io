---
layout: post
title: "Feature superposition, co-occurrence and output influence in LLMs: a sparse autoencoder-based analysis"
date: 2024-10-20 12:00:00 -0500
excerpt: "Examining the effect of feature co-occurrence and downstream output influence on feature superposition."
importance: 2
github: "https://github.com/armaan-abraham/Qres"
---

There seems to be tension in the literature between two phenomena of feature
superposition in neural networks, which was (and still kind of is) confusing to
me.

(1) From Anthropic's [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html):

- Two features that _do not_ co-occur will take similar bases in activation
space (but are measured in opposite directions) and are decoupled with the
nonlinear activation function.
- __Two feature that _do_ co-occur will take orthogonal bases in activation space.__ They will not share a basis as doing so leads to interference when the features occur together.

(2) On the other hand, from Anthropic's [Toward Monosemanticity](https://transformer-circuits.pub/2023/monosemantic-features#phenomenology-feature-splitting):

- __Two features which correlate to similar downstream neural network outputs
(i.e. have similar "output actions") will take similar bases in activation
space.__ This is because isolating these features is not important when they
both ultimately correspond to the same output action; some interference is okay.

However, these two phenomena are somewhat contradictory because wouldn't we
expect that features that co-occur will also have similar output actions? The
authors of (2) acknowledge this tension.

I attempted to resolve these phenomena by training an SAE on a 6.3M parameter
LLM. [Here is my
analysis](https://github.com/armaan-abraham/sparse-autoencoder/blob/master/analysis.ipynb).
I discovered a rough heuristic for the cosine between the directions of any two
features that considers both correlation _and_ downstream output similarity.  Here
are the rough results:

|                                   | features DO NOT cooccur | features DO cooccur |
| --------------------------------- | ------------------- | ----------------------- |
| __output distribution SIMILAR__   | cos < 0            | cos > 0                 |
| __output distribution DIFFERENT__ | cos < 0            | cos = 0                |

Which was gathered from this plot:
<div style="text-align: center;">
  <img src="/assets/images/feature-superposition.png" alt="feature-superposition">
</div>

This makes some sense:
- If two features cooccur but yield different outputs, they must take mutually
orthogonal bases to avoid interference.
- If two features cooccur and have the same output, then they can take a similar
basis because it's okay to have interference between them (as they both yield
the same output).
- If two features do not cooccur, then they can share a basis (superposition),
such as by forming [antipodal
pairs](https://transformer-circuits.pub/2022/toy_model/index.html#geometry-uniform).

What's still a bit confusing:
- Why do we see such a big splotch of orthogonal feature directions? There can
only be so many orthogonal pairs, which is why we see superposition in the first
place. Maybe, our analysis needs to be more sensitive to small alignments /
antialignments.
- Why would the model choose to use superposition for features that do not
cooccur, but cause the same output? Although superposition saves capacity, it
costs even fewer parameters to just share the exact same basis (i.e. with the
same sign too).


