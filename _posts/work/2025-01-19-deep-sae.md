---
layout: post
title: "Deep sparse autoencoders for neural network interpretability"
date: 2025-01-19 12:00:00 -0500
excerpt: "What if we just add more layers to sparse autoencoders?"
importance: 6
github: "https://github.com/armaan-abraham/mlsae"
---

[Github](https://github.com/armaan-abraham/mlsae)

<div style="text-align: center;">
  <img src="/assets/images/deep-sae-frontier.png" alt="frontier" style="max-width: 500px;">
  <figcaption style="text-align: center; font-style: italic;">Fig 1. Adding hidden layers to SAEs improves the sparsity-reconstruction frontier.</figcaption>
</div>

## Motivations and intuitions

- Zico Kolter and thinking on the wrong level of abstraction
- What does it mean to be interpretable? The workings of the network are already manifest in the weights and activations.
- Yoshua Bengio and the global workspace theory
- The dark matter of sparse autoencoders
- What if, by developing sparse autoencoders, the important lesson we stumbled upon is not that linear features represent some 'true' explanation for how neural networks operate, but that sparsity coincides with interpretability, in a mechanism-agnostic way?
- Incompleteness of and tension in the theory of feature superposition
    - Building on my [previous work]({% post_url 2024-10-20-sae %}) on feature superposition in sparse autoencoders

## Results

- Issues with sparse optimization problems
- Improvement of frontier
  - But this was kind of obvious
- The more important thing to investigate is feature interpretability

## In progress

- Automated interpretability scores
- Manual feature analysis