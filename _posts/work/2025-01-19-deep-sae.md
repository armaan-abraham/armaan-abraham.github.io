---
layout: post
title: "Deep sparse autoencoders for neural network interpretability"
date: 2025-01-19 12:00:00 -0500
excerpt: "What if we just add more layers to sparse autoencoders?"
importance: 6
github: "https://github.com/armaan-abraham/mlsae"
---

[Github](https://github.com/armaan-abraham/mlsae)

<div style="text-align: center;">
  <img src="/assets/images/deep-sae-frontier.png" alt="frontier" style="max-width: 500px;">
  <figcaption style="text-align: center; font-style: italic;">Fig 1. Adding hidden layers to SAEs improves the sparsity-reconstruction frontier.</figcaption>
</div>

## Motivations and intuitions

#### Levels of abstraction

At NeurIPS 2024, Zico Kolter gave a talk titled "Is this really science? A
lukewarm defense of alchemy". In it, someone (I think it might have been Surya
Ganguli, but I'm not confident about this) asked him if he thinks we could ever
understand neural networks in a scientific way. To the best of my memory, Zico
responded with something along the lines of yes, but that he doesn't think we're
thinking about deep learning science at the right level of abstraction. I
believe he also said something about how thinking at a higher level of
abstraction to reason about neural networks might not look like "science" as
we're used to it. The recording of this talk is posted
[here](https://neurips.cc/virtual/2024/107918), but I'm too lazy to go back and
confirm my paraphrasing.

I think that this question about how to think at the right level of abstraction
is particularly relevant for deep learning, and especially deep learning
interpretability. After all, when we say that we want to "understand" how a
neural network really operates, why are we not satisfied with the weights of the
neural network, of which we are virtually certain? I think this could partially come
down to translating these mechanisms into a level of abstraction that our human
minds can operate on.

#### Sparse autoencoders

[Sparse
autoencoders](https://transformer-circuits.pub/2023/monosemantic-features) have
been a remarkably successful approach to understanding the internal states of a
neural network. The question of *why* they work so well has interested me quite
a bit. 

The standard theory behind the success of SAEs (and motivation behind the
initiation of the project) is that neural networks fundamentally operate by
storing linear "features", which are just directions in neuron activation space,
which are then transformed through the forward pass of the network. To store
more features than the network has neurons, the network employs "superposition",
whereby a single neuron can represent multiple features. The sparse autoencoder
works, then, by imposing a sparsity constraint that separates these features and
extracts them out of their superposed state, and thus allows us to examine and
interpret them. This theory is laid out in Anthropic's [Toy models of
superposition work](https://transformer-circuits.pub/2022/toy_model/index.html).

However, despite their success, there are flaws in both SAEs, as well as the
theory that seeks to explain it. For one, there are various competing hypotheses
about what determines what directions in activation space features should take,
which I [previously examined]({% post_url 2024-10-20-sae %}). 
There also seems to be some variation in activation space that cannot be
explained by SAEs, and there is [some indication that making SAEs wider will
not fix this problem](https://arxiv.org/abs/2410.14670). 

#### Sparsity in tensor learning and the global workspace theory

Interestingly, sparsity is not only involved in interpreting neural networks via
SAEs, but sparsity is also used in other fields to help understand complex
systems. For example, sparsity constraints are added to tensor decomposition
methods in biology to help yield more interpretable factors and, ultimately, to
better elucidate biological mechanisms ([example
1](https://pubmed.ncbi.nlm.nih.gov/32841133/), [example
2](https://pubmed.ncbi.nlm.nih.gov/29589554/)). I supposed you could argue that
these systems exhibit superposition too, and that is what the sparsity is
helping with, but I think this is somewhat of a stretch. I think that
cross-domain examples point to sparsity as being fundamentally correlated to
interpretability in some way.

One way you may be able to look at this is through the lens of Global Workspace
Theory (GWT). GWT is a concept from cognitive science that describes the human
mind as containing a conscious "theater", which has a limit to the number of
abstractions it can handle at once, often described as 5-7. Abstractions are
filtered from other parts of the brain (the global workspace) before entering
the theater. I originally learned about GWT through Yoshua Bengio's writing, for
example this paper on [The Consciousness
Prior](https://arxiv.org/abs/1709.08568) (where, btw, involves a *sparse* factor
graph). I relate this to neural network interpretability in the sense that
analyzing an entire weight matrix or activation vector as being difficult
because of the sheer number of abstractions it contains, and that they don't fit
in the "theater". Sparsity, then, is about creating fewer abstractions at a
higher level that can then fit in the theater.

My question is: *what if the main lesson from the success of SAEs is not that
linear features in superposition are the fundamental components of neural
network operation, but that interpretability is fundamentally related to
building sparse, yet faithful, representations of an arbitrary mechanism?*

## Results

If my philosophizing is accurate, this would imply that adding more
hidden layers to SAEs would yield more interpretable and faithful explanations
of neural network function. So, this is what I'm working on testing. 

#### Optimization challenges with sparsity

Apparently, you can't use Adam for sparse optimization problems, because of the
first and second moment tracking! Dead neurons are generally a difficulty.

#### Improvement of the pareto frontier

So far, as per Fig 1, adding hidden layers seems to improve the esteemed L0-MSE
frontier, which is a common measure of SAE performance. These results were for
the extremely simple tiny-stories-3M model, and I think this is the cause for
some of the weirdness (e.g., some deep SAEs achieve a pretty good MSE by having
fewer than 1 latent activation on average). I plan to scale this to GPT-2 next.

## In progress

But, this improvement of the L0-MSE frontier is not suprising to be honest. The
real challenge is whether the features produced by deep SAEs are as
interpretable as those produced by shallow SAEs. I'm currently working on
quantifying this through automated interpretability measures.